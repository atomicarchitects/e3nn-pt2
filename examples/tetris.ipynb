{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tinygrad import Tensor, dtypes\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Disable future warnings.\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad import Device\n",
    "Device.DEFAULT = \"CUDA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition for left and right screws.\n",
    "tetracubes = np.array([\n",
    "  # Right screw.\n",
    "  [ [ -0.50,  0.25, -0.25 ], [ -0.50,  0.25,  0.75 ],\n",
    "    [  0.50,  0.25, -0.25 ], [  0.50, -0.75, -0.25 ] ],\n",
    "  # Left screw.\n",
    "  [ [ -0.75,  0.50, -0.25 ], [  0.25, -0.50,  0.75 ],\n",
    "    [  0.25,  0.50, -0.25 ], [  0.25, -0.50, -0.25 ] ],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_datasets(num_train=1000, num_valid=100, noise_scale=0.05):\n",
    "\n",
    "  # Assign a label to each tetracube.\n",
    "  labels = np.arange(tetracubes.shape[0])\n",
    "\n",
    "  # Randomly choose among the eight tetracubes to generate train and validation datasets.\n",
    "  train_choice = np.random.choice(tetracubes.shape[0], size=(num_train,))\n",
    "  valid_choice = np.random.choice(tetracubes.shape[0], size=(num_valid,))\n",
    "  train_shapes = Tensor(np.take(tetracubes, train_choice, axis=0), dtype=dtypes.float32, requires_grad=True)\n",
    "  valid_shapes = Tensor(np.take(tetracubes, valid_choice, axis=0), dtype=dtypes.float32, requires_grad=True)\n",
    "  train_labels = Tensor(np.take(labels, train_choice, axis=0), dtype=dtypes.float32)\n",
    "  valid_labels = Tensor(np.take(labels, valid_choice, axis=0), dtype=dtypes.float32)\n",
    "  \n",
    "  # Add Gaussian noise for some variety.\n",
    "  train_shapes = train_shapes + noise_scale * Tensor(np.random.normal(size=train_shapes.shape), dtype=dtypes.float32)\n",
    "  valid_shapes = valid_shapes + noise_scale * Tensor(np.random.normal(size=valid_shapes.shape), dtype=dtypes.float32)\n",
    "  \n",
    "  # Return final train and validation datasets.\n",
    "  train_data = dict(shapes=train_shapes.realize(), labels=train_labels.realize())\n",
    "  valid_data = dict(shapes=valid_shapes.realize(), labels=valid_labels.realize())\n",
    "  return train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional, Protocol, Tuple, Union, List\n",
    "from tinygrad import Tensor\n",
    "\n",
    "def basis(\n",
    "    r: Tensor,\n",
    "    max_degree: int,\n",
    "    num: int,\n",
    "    # radial_fn: Callable[[Float[Array, '...'], int], Float[Array, '... num']],\n",
    "):\n",
    "    r\"\"\"Basis function corresponding to e3x.nn.basis which uses\n",
    "        e3nn.spherical_harmonics for angular functions\n",
    "    \"\"\"\n",
    "    \n",
    "    original_shape = r.shape[:-1]\n",
    "    r = r.reshape(-1, 3)\n",
    "\n",
    "    # Normalize input vectors.\n",
    "    a = torch.maximum(torch.max(torch.abs(r)), torch.finfo(r.dtype).tiny)\n",
    "    b = r / a\n",
    "    norm = a * torch.sqrt(torch.sum(b * b, dim=-1, keepdim=True))\n",
    "    u = r / torch.where(norm > 0, norm, 1)\n",
    "    norm = norm.squeeze(-1)  # (...)\n",
    "\n",
    "    # radial function\n",
    "    # rbf = radial_fn(norm, num)  # (..., N)\n",
    "\n",
    "    # basis function\n",
    "    ylm = e3nn.spherical_harmonics(e3nn.s2_irreps(max_degree), u, normalize=\"component\")\n",
    "    return ylm\n",
    "    \n",
    "    product = lambda x, weight: lambda w: w * x(weight)(ylm, rbf)\n",
    "\n",
    "    return product.reshape((*original_shape, *product.shape[-2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1802968081.py, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[83], line 19\u001b[0;36m\u001b[0m\n\u001b[0;31m    tp =\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class TensorDense(torch.nn.Module):\n",
    "\n",
    "    features: int\n",
    "    max_degree: int\n",
    "    irreps_out: e3nn.Irreps\n",
    "    use_gaunt: bool\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: e3nn.IrrepsArray) -> e3nn.IrrepsArray:\n",
    "\n",
    "        x1 = tinygrad.nn.Linear(irreps_in=x.irreps, irreps_out=x.irreps, channel_out=self.features)(x)\n",
    "        x2 = tinygrad.nn.Linear(irreps_in=x.irreps, irreps_out=x.irreps, channel_out=self.features)(x)\n",
    "            \n",
    "        # Keep irreps only up to max_degree.\n",
    "        filter_ir_out = e3nn.tensor_product(x1.irreps, x2.irreps).filter(\n",
    "            lmax=self.max_degree\n",
    "        )\n",
    "    \n",
    "        tp = \n",
    "\n",
    "        # Additionally, filter out irreps.\n",
    "        irreps_out = self.irreps_out\n",
    "        if irreps_out is None:\n",
    "            irreps_out = tp.irreps\n",
    "\n",
    "        x = e3nn.o3.Linear(irreps_in=tp.irreps_out, irreps_out=irreps_out)(tp)        \n",
    "        return x\n",
    "\n",
    "class E3NNModel(torch.nn.Module):\n",
    "  features = 8\n",
    "  max_degree = 3\n",
    "  use_gaunt: bool = False\n",
    "\n",
    "  def __call__(self, shapes):  # The 'shapes' array has shape (..., 4, 3).\n",
    "      \n",
    "    # 1. Center shapes at origin (for translational invariance).\n",
    "    shapes -= torch.mean(shapes, keepdims=True, axis=-2)   # 'shapes' still has shape (..., 4, 3).\n",
    "    \n",
    "    # 2. Featurize by expanding cube midpoints in basis functions and taking the mean over the 4 cubes.\n",
    "    \n",
    "    x = basis( \n",
    "      shapes,\n",
    "      num=self.features,\n",
    "      max_degree=self.max_degree,\n",
    "    #   radial_fn=functools.partial(e3x.nn.triangular_window, limit=2.0),\n",
    "    ) # 'x' has shape (..., 4, (max_degree+1)**2, features).\n",
    "    \n",
    "    x = e3nn.mean(x, axis=-3)  # 'x' now has shape (..., (max_degree+1)**2, features).\n",
    "        \n",
    "    # 3. Apply feature transformations.\n",
    "        \n",
    "    \"No pseudoscalar features yet\"\n",
    "    x = TensorDense(\n",
    "            features=self.features, max_degree=self.max_degree, irreps_out=None, use_gaunt=self.use_gaunt\n",
    "        )(x)\n",
    "    \n",
    "    \"Pseduoscalar features in tensor product\"\n",
    "    x = TensorDense(\n",
    "            features=self.features, max_degree=self.max_degree, irreps_out=\"0e + 0o\", use_gaunt=self.use_gaunt\n",
    "        )(x)\n",
    "\n",
    "    x = x.axis_to_mul()\n",
    "    # 4. Predict logits (with an ordinary Dense layer).\n",
    "    logits = torch.nn.Linear(features=tetracubes.shape[0])(x.array)  # Logits has shape (..., 2).\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad import nn, TinyJit\n",
    "\n",
    "class OrdinaryModel:\n",
    "  features = 8\n",
    "  \n",
    "  def __init__(self):\n",
    "      self.layers: List[Callable[[Tensor], Tensor]] = [\n",
    "        nn.Linear(12, self.features), Tensor.relu, # 'x' has shape (..., features).\n",
    "        nn.Linear(self.features, self.features), Tensor.relu,\n",
    "        nn.Linear(self.features, tetracubes.shape[0])] \n",
    "        \n",
    "  def __call__(self, shapes):  # The 'shapes' array has shape (..., 4, 3).\n",
    "    # 1. Center shapes at origin (for translational invariance).\n",
    "    shapes = shapes - Tensor.mean(shapes, keepdim=True, axis=-2)  # 'shapes' still has shape (..., 4, 3).\n",
    "\n",
    "    # 2. Flatten xyz coordinates (input features).\n",
    "    x = Tensor.reshape(shapes, (*shapes.shape[:-2], -1))  # 'x' has shape (..., 4*3).\n",
    "\n",
    "    return x.sequential(self.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x, num_classes:int) -> Tensor:\n",
    "    return (x[..., None] == Tensor.arange(num_classes, requires_grad=False, device=x.device)).where(1, 0)\n",
    "\n",
    "def compute_loss(logits, labels):\n",
    "    labels_hot = one_hot(labels, tetracubes.shape[0])\n",
    "    loss = logits.sparse_categorical_crossentropy(labels_hot).mean()\n",
    "    return loss\n",
    "\n",
    "    \n",
    "def compute_accuracy(logits, labels):\n",
    "    return Tensor.mean(Tensor.argmax(logits, -1) == labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "@TinyJit\n",
    "def train_step(model, optimizer, batch):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(batch[\"shapes\"])\n",
    "    labels = batch[\"labels\"]\n",
    "    loss = compute_loss(logits, labels).backward()\n",
    "    optimizer.step()\n",
    "    accuracy = compute_accuracy(logits, labels)\n",
    "    return loss.item(), accuracy.item()\n",
    "\n",
    "@TinyJit\n",
    "def eval_step(model, batch):\n",
    "    logits = model(batch[\"shapes\"])\n",
    "    labels = batch[\"labels\"]\n",
    "    loss = compute_loss(logits, labels)\n",
    "    accuracy = compute_accuracy(logits=logits, labels=labels)\n",
    "    return loss.item(), accuracy.item()\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model, train_data, valid_data, num_epochs, learning_rate, batch_size\n",
    "):\n",
    "    # Initialize model parameters and optimizer state.\n",
    "    \n",
    "    optimizer = nn.optim.Adam(nn.state.get_parameters(model), lr=learning_rate)\n",
    "\n",
    "    # Determine the number of training steps per epoch.\n",
    "    train_size = train_data[\"shapes\"].shape[0]\n",
    "    steps_per_epoch = train_size // batch_size\n",
    "\n",
    "    # Train for 'num_epochs' epochs.\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Draw random permutations for fetching batches from the train data.\n",
    "        perms = Tensor(np.random.permutation(train_size))\n",
    "        perms = perms[\n",
    "            : steps_per_epoch * batch_size\n",
    "        ]  # Skip the last batch (if incomplete).\n",
    "        perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "        # Loop over all batches.\n",
    "        train_loss = 0.0  # For keeping a running average of the loss.\n",
    "        train_accuracy = 0.0  # For keeping a running average of the accuracy.\n",
    "        for i, perm in enumerate(perms):\n",
    "            batch = {k: v[perm, ...] for k, v in train_data.items()}\n",
    "            if i == 0:\n",
    "                print(\"batch\", batch[\"shapes\"].shape)\n",
    "            loss, accuracy = train_step(\n",
    "                model=model,\n",
    "                optimizer=optimizer,\n",
    "                batch=batch,\n",
    "            )\n",
    "            train_loss = train_loss + (loss - train_loss) / (i + 1)\n",
    "            train_accuracy = train_accuracy + (accuracy - train_accuracy) / (i + 1)\n",
    "\n",
    "            # Evaluate on the test set after each training epoch.\n",
    "            valid_loss, valid_accuracy = eval_step(\n",
    "                model=model, batch=valid_data\n",
    "            )\n",
    "\n",
    "            # Print progress.\n",
    "            print(f\"epoch: {epoch}\")\n",
    "            print(\n",
    "                f\"  train: loss {train_loss : 4.2f}, accuracy {train_accuracy * 100 : 6.2f}%\"\n",
    "            )\n",
    "            print(\n",
    "                f\"  valid: loss {valid_loss : 4.2f}, accuracy {valid_accuracy * 100 : 6.2f}%\"\n",
    "            )\n",
    "\n",
    "    # Return final model parameters.\n",
    "    return nn.state.get_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate train and test datasets.\n",
    "train_data, valid_data = generate_datasets()\n",
    "\n",
    "# Define training hyperparameters.\n",
    "learning_rate = 0.005\n",
    "num_epochs = 5\n",
    "batch_size = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch (15, 4, 3)\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 1\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "batch (15, 4, 3)\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 2\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "batch (15, 4, 3)\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 3\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "batch (15, 4, 3)\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 4\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "batch (15, 4, 3)\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n",
      "epoch: 5\n",
      "  train: loss  0.68, accuracy  56.25%\n",
      "  valid: loss  0.69, accuracy  42.00%\n"
     ]
    }
   ],
   "source": [
    "# Train the ordinary model.\n",
    "ordinary_model = OrdinaryModel()\n",
    "\n",
    "ordinary_params = train_model(\n",
    "    model=ordinary_model,\n",
    "    train_data=train_data,\n",
    "    valid_data=valid_data,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from https://e3x.readthedocs.io/stable/_modules/e3x/so3/rotations.html#random_rotation\n",
    "\n",
    "def random_rotation(\n",
    "    perturbation: float = 1.0,\n",
    "    num: int = 1,  # When num=1, leading dimension is automatically squeezed.\n",
    "):\n",
    "  r\"\"\"Samples a random :math:`3\\times3` rotation matrix.\n",
    "\n",
    "  Samples random :math:`3\\times3` rotation matrices from :math:`\\mathrm{SO(3)}`.\n",
    "  The ``perturbation`` parameter controls how strongly random points on a sphere\n",
    "  centered on the origin are perturbed by the rotation. For\n",
    "  ``perturbation=1.0``, any point on the sphere is rotated to any other point on\n",
    "  the sphere with equal probability. If ``perturbation<1.0``, returned rotation\n",
    "  matrices are biased to identity matrices. For example, with\n",
    "  ``perturbation=0.5``, a point on the sphere is rotated to any other point on\n",
    "  the same hemisphere with equal probability.\n",
    "\n",
    "  Example:\n",
    "    >>> import jax\n",
    "    >>> import e3x\n",
    "    >>> tinye3nn.so3.random_rotation(jax.random.PRNGKey(0), perturbation=1.0)\n",
    "    Array([[-0.93064284, -0.11807037,  0.34635717],\n",
    "           [ 0.33270139,  0.1210826 ,  0.9352266 ],\n",
    "           [-0.15236041,  0.9855955 , -0.07340252]], dtype=float32)\n",
    "    >>> tinye3nn.so3.random_rotation(jax.random.PRNGKey(0), perturbation=0.0)\n",
    "    Array([[1., 0., 0.],\n",
    "           [0., 1., 0.],\n",
    "           [0., 0., 1.]], dtype=float32)\n",
    "\n",
    "  Args:\n",
    "    key: A PRNG key used as the random key.\n",
    "    perturbation: A value between 0.0 and 1.0 that determines the perturbation.\n",
    "    num: Number of returned rotation matrices.\n",
    "\n",
    "  Returns:\n",
    "    An Array of shape :math:`(\\mathrm{num}, 3, 3)` or :math:`(3, 3)` (if num =\n",
    "    1) representing random :math:`3\\times3` rotation matrices.\n",
    "  \"\"\"\n",
    "  # Check that perturbation is a meaningful value.\n",
    "  if not 0.0 <= perturbation <= 1.0:\n",
    "    raise ValueError(\n",
    "        f'perturbation must be between 0.0 and 1.0, received {perturbation}'\n",
    "    )\n",
    "  # Draw random numbers and transform them.\n",
    "  twopi = 2 * np.pi\n",
    "  u = np.random.uniform(size=(num, 3))\n",
    "  sqrt1 = np.sqrt(1 - u[..., 0])\n",
    "  sqrt2 = np.sqrt(u[..., 0])\n",
    "  angl1 = twopi * u[..., 1]\n",
    "  angl2 = twopi * u[..., 2]\n",
    "  # Construct random quaternion.\n",
    "  r = sqrt1 * np.sin(angl1)\n",
    "  i = sqrt1 * np.cos(angl1)\n",
    "  j = sqrt2 * np.sin(angl2)\n",
    "  k = sqrt2 * np.cos(angl2)\n",
    "  # Perturbation (Slerp starting from identity quaternion).\n",
    "  flip = r < 0  # Flip sign if r < 0 (always take the shorter route).\n",
    "  r = np.where(flip, -r, r)\n",
    "  i = np.where(flip, -i, i)\n",
    "  j = np.where(flip, -j, j)\n",
    "  k = np.where(flip, -k, k)\n",
    "  phi = np.arccos(r)\n",
    "  sinphi = np.sin(phi)\n",
    "  # Prevent division by zero.\n",
    "  zeromask = np.abs(sinphi) < 1e-9\n",
    "  f1 = np.where(\n",
    "      zeromask, 1 - perturbation, np.sin((1 - perturbation) * phi) / sinphi\n",
    "  )\n",
    "  f2 = np.where(zeromask, perturbation, np.sin(perturbation * phi) / sinphi)\n",
    "  r, i, j, k = f1 + f2 * r, f2 * i, f2 * j, f2 * k\n",
    "  # Construct rotation matrix.\n",
    "  i2, j2, k2 = i * i, j * j, k * k\n",
    "  ij, ik, jk, ir, jr, kr = i * j, i * k, j * k, i * r, j * r, k * r\n",
    "  row1 = np.stack((1 - 2 * (j2 + k2), 2 * (ij - kr), 2 * (ik + jr)), axis=-1)\n",
    "  row2 = np.stack((2 * (ij + kr), 1 - 2 * (i2 + k2), 2 * (jk - ir)), axis=-1)\n",
    "  row3 = np.stack((2 * (ik - jr), 2 * (jk + ir), 1 - 2 * (i2 + j2)), axis=-1)\n",
    "  rot = np.squeeze(np.stack((row1, row2, row3), axis=-1))\n",
    "  return Tensor(rot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the accuracy of the equivariant and ordinary models for rotated shapes with different\n",
    "# perturbation magnitudes (a perturbation of 0.0 gives identity matrices, a perturbation of 1.0 gives\n",
    "# fully random rotation matrices).\n",
    "perturbations = np.linspace(0.0, 1.0, num=6)\n",
    "ordinary_accuracy = np.zeros(len(perturbations))\n",
    "\n",
    "for i, perturbation in enumerate(perturbations):\n",
    "  rot = random_rotation(perturbation=perturbation, num=valid_data['shapes'].shape[0])\n",
    "  rotated_shapes = valid_data['shapes']@rot\n",
    "  rotated_valid_data = {'shapes': rotated_shapes, 'labels': valid_data['labels']}\n",
    "#   _, equivariant_accuracy[i] = eval_step(equivariant_model.apply, rotated_valid_data, equivariant_params)\n",
    "  _, ordinary_accuracy[i] = eval_step(ordinary_model, rotated_valid_data)\n",
    "#   _, e3nn_accuracy[i] = eval_step(e3nn_model.apply, rotated_valid_data, e3nn_params)\n",
    "#   _, gtp_accuracy[i] = eval_step(gtp_model.apply, rotated_valid_data, gtp_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the results.\n",
    "plt.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
    "plt.xlabel('rotational perturbation [a.u.]')\n",
    "plt.ylabel('accuracy [%]')\n",
    "plt.plot(perturbations, ordinary_accuracy * 100, label='simple mlp model')\n",
    "plt.hlines(y=100/tetracubes.shape[0], xmin=0.0, xmax=1.0, color='k', ls=':', label='random')\n",
    "plt.legend(loc='center right')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
